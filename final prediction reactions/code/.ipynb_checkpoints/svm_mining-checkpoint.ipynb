{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions=['love','wow','angry','sad','care']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/Data_cleaning_with_content.csv\")\n",
    "data=data.dropna()\n",
    "contents=data['contentclean'].values\n",
    "love=data['love'].values\n",
    "wow=data['wow'].values\n",
    "sad=data['sigh'].values\n",
    "angry=data['grrr'].values\n",
    "care=data['care'].values\n",
    "category=data['category'].values\n",
    "sum_reaction=data['sumreactions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "love=love/sum_reaction\n",
    "wow=wow/sum_reaction\n",
    "angry=angry/sum_reaction\n",
    "sad=sad/sum_reaction\n",
    "care=care/sum_reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotions=np.array([love]).reshape(-1,1)\n",
    "# Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Emotions=np.insert(Emotions,0,love,axis=1)\n",
    "Emotions=np.insert(Emotions,1,wow,axis=1)\n",
    "Emotions=np.insert(Emotions,2,angry,axis=1)\n",
    "Emotions=np.insert(Emotions,3,sad,axis=1)\n",
    "Emotions=np.insert(Emotions,4,care,axis=1)\n",
    "# Emotion_mining=np.array(Emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20443, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent Data Using TFIDF For SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train):\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(min_df=2, max_df=0.3, ngram_range=(2, 3))\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "    X_train=tfidf_vectorizer.transform(X_train)\n",
    "#     X_test=tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train,tfidf_vectorizer\n",
    "#, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulid SVC For Emotion Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encodin_label(ytrain):\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_nn = encoder.fit_transform(ytrain)\n",
    "    return y_train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Accidents Crimes', 'Art Culture', 'Economy', 'Education',\n",
       "       'Entertainment', 'Health', 'Military', 'Politics', 'Religion',\n",
       "       'Science Technology', 'Social', 'Sport', 'Tourism'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(category)\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "finished 239.71499395370483\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import time\n",
    "clf_Emotion = svm.SVR()\n",
    "# clf_Emotion.probability=True\n",
    "clf_Sentiment = svm.SVC(kernel='precomputed')\n",
    "clf_Sentiment.probability=True\n",
    "clf_Category = svm.SVC()\n",
    "clf_Category.probability=True\n",
    "def train_svc(clf,x_train,y_train):\n",
    "    X_train,tfidfvectorize=tfidf_features(x_train)\n",
    "#     Y_train=encodin_label(y_train)\n",
    "    s=time.time()\n",
    "    print(\"-\"*10)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(\"finished\",time.time()-s)\n",
    "    return clf,tfidfvectorize\n",
    "def predict_svm(clf,post,tfidf):\n",
    "    post=tfidf_vectorizer.transform([post])\n",
    "    return clf.predict_proba(post)\n",
    "    \n",
    "clf_Emotion,TF_emotion=train_svc(clf_Emotion,contents,care)\n",
    "# clf_Sentiment,TF_Sentiment=train_svc(clf_Sentiment,X_train,Y_train)\n",
    "# clf_Category,TF_Category=train_svc(clf_Category,X_train,Y_train)\n",
    "def get_all_vectors_for_post_emotion_category_sentiment(post):\n",
    "    return predict_svm(clf_Emotion,post,TF_emotion),predict_svm(clf_Sentiment,post,TF_Sentiment),predict_svm(clf_Category,post,TF_Category)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"SVC_care.pickle\",\"wb\") as file:\n",
    "    pickle.dump(clf_Emotion,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TF_Vecotrize.pickle\",\"wb\") as file:\n",
    "    pickle.dump(TF_emotion,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def convert_one_hot(ytrain):\n",
    "#     encoder = LabelEncoder()\n",
    "    y_train_nn = encoder.transform(ytrain)\n",
    "    y_train_nn\n",
    "    encoders = OneHotEncoder(sparse=False)\n",
    "    y_train_nn = y_train_nn.reshape(len(y_train_nn), 1)\n",
    "    y_train_nn=encoders.fit_transform(y_train_nn)\n",
    "    return y_train_nn\n",
    "Y_train=convert_one_hot(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20443, 13)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen=100\n",
    "tokenizer = Tokenizer(num_words=15000)\n",
    "\n",
    "tokenizer.fit_on_texts(contents)\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(contents)\n",
    "# X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "    # Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Pad sequences with zeros\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 100, 50)           3357900   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 96, 128)           32128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 3,391,705\n",
      "Trainable params: 3,391,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 50, input_length=100))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(13,activation=\"softmax\"))\n",
    "model.compile(optimizer='adam',\n",
    "                loss='mse',\n",
    "                 metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "160/160 [==============================] - 23s 136ms/step - loss: 0.0678 - accuracy: 0.2182\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 21s 129ms/step - loss: 0.0645 - accuracy: 0.2846\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 20s 126ms/step - loss: 0.0610 - accuracy: 0.3531\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 20s 125ms/step - loss: 0.0551 - accuracy: 0.4519\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 20s 125ms/step - loss: 0.0477 - accuracy: 0.5338\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 20s 128ms/step - loss: 0.0423 - accuracy: 0.5807\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 20s 125ms/step - loss: 0.0384 - accuracy: 0.6101\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 20s 126ms/step - loss: 0.0352 - accuracy: 0.6602\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 20s 128ms/step - loss: 0.0328 - accuracy: 0.6923\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 20s 128ms/step - loss: 0.0304 - accuracy: 0.7080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ba90228c08>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,batch_size=128,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN_category.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer_NN.pickle\",\"wb\") as file:\n",
    "    pickle.dump(tokenizer,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3491851e-03, 8.1063056e-04, 1.1725579e-03, 3.2122436e-04,\n",
       "        6.5769744e-04, 2.0318530e-03, 3.0358115e-04, 4.8772068e-05,\n",
       "        1.6519595e-03, 5.3208960e-06, 9.9078685e-01, 8.4135431e-04,\n",
       "        1.8980825e-05]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20057, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
