{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reactions=['love','wow','angry','sad','care']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/Data_cleaning_with_content.csv\")\n",
    "data=data.dropna()\n",
    "contents=data['contentclean'].values\n",
    "love=data['love'].values\n",
    "wow=data['wow'].values\n",
    "sad=data['sigh'].values\n",
    "angry=data['grrr'].values\n",
    "care=data['care'].values\n",
    "category=data['category'].values\n",
    "sum_reaction=data['sumreactions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "love=love/sum_reaction\n",
    "wow=wow/sum_reaction\n",
    "angry=angry/sum_reaction\n",
    "sad=sad/sum_reaction\n",
    "care=care/sum_reaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotions_svr={\"love\":love,\"wow\":wow,\"angry\":angry,\"sad\":sad,\"care\":care}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotions=np.array([love]).reshape(-1,1)\n",
    "# Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Emotions=np.insert(Emotions,0,love,axis=1)\n",
    "Emotions=np.insert(Emotions,1,wow,axis=1)\n",
    "Emotions=np.insert(Emotions,2,angry,axis=1)\n",
    "Emotions=np.insert(Emotions,3,sad,axis=1)\n",
    "Emotions=np.insert(Emotions,4,care,axis=1)\n",
    "# Emotion_mining=np.array(Emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20443, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emotions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent Data Using TFIDF For SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train):\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(min_df=2, max_df=0.3, ngram_range=(2, 3))\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "    X_train=tfidf_vectorizer.transform(X_train)\n",
    "#     X_test=tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train,tfidf_vectorizer\n",
    "#, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulid SVR For Emotion Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encodin_label(ytrain):\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_nn = encoder.fit_transform(ytrain)\n",
    "    return y_train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(category)\n",
    "# encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import time\n",
    "clf_Emotion = svm.SVR()\n",
    "# clf_Emotion.probability=True\n",
    "# clf_Sentiment = svm.SVC(kernel='precomputed')\n",
    "# clf_Sentiment.probability=True\n",
    "clf_Category = svm.SVC()\n",
    "clf_Category.probability=True\n",
    "def train_svc(clf,x_train,y_train):\n",
    "    X_train,tfidfvectorize=tfidf_features(x_train)\n",
    "#     Y_train=encodin_label(y_train)\n",
    "    s=time.time()\n",
    "    print(\"-\"*10)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(\"finished\",time.time()-s)\n",
    "    return clf,tfidfvectorize\n",
    "def predict_svm(clf,post,tfidf):\n",
    "    post=tfidf_vectorizer.transform([post])\n",
    "    return clf.predict_proba(post)\n",
    "    \n",
    "# clf_Emotion,TF_emotion=train_svc(clf_Emotion,contents,care)\n",
    "# clf_Sentiment,TF_Sentiment=train_svc(clf_Sentiment,X_train,Y_train)\n",
    "# clf_Category,TF_Category=train_svc(clf_Category,X_train,Y_train)\n",
    "def get_all_vectors_for_post_emotion_category_sentiment(post):\n",
    "    return predict_svm(clf_Emotion,post,TF_emotion),predict_svm(clf_Sentiment,post,TF_Sentiment),predict_svm(clf_Category,post,TF_Category)\n",
    "\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "finished 315.6224138736725\n",
      "----------\n",
      "finished 291.98644256591797\n",
      "----------\n",
      "finished 270.83100867271423\n",
      "----------\n",
      "finished 330.81699562072754\n",
      "----------\n",
      "finished 227.58900356292725\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "def train_svm():\n",
    "    i=0\n",
    "    for k,v in Emotions_svr.items():\n",
    "        \n",
    "        clf_Emotion_,TF_emotion_=train_svc(clf_Emotion,contents,v)\n",
    "        with open(\"../models/SVR_\"+str(k)+\".pickle\",\"wb\") as file:\n",
    "            pickle.dump(clf_Emotion_,file)\n",
    "        if i<1:\n",
    "            i=i+1\n",
    "            \n",
    "            with open(\"../models/TF_Vecotrize.pickle\",\"wb\") as file:\n",
    "\n",
    "                pickle.dump(TF_emotion_,file)\n",
    "        \n",
    "    clf_Emotion_,TF_emotion_=train_svc(clf_Category,contents,category)\n",
    "    with open(\"../models/SVC_category.pickle\",\"wb\") as file:\n",
    "        pickle.dump(clf_Emotion_,file)\n",
    "\n",
    "train_svm()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Cnn Lstm Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def convert_one_hot(ytrain):\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_nn = encoder.fit_transform(ytrain)\n",
    "    y_train_nn\n",
    "    encoders = OneHotEncoder(sparse=False)\n",
    "    y_train_nn = y_train_nn.reshape(len(y_train_nn), 1)\n",
    "    y_train_nn=encoders.fit_transform(y_train_nn)\n",
    "    return y_train_nn\n",
    "maxlen=100\n",
    "tokenizer = Tokenizer(num_words=15000)\n",
    "\n",
    "tokenizer.fit_on_texts(contents)\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(contents)\n",
    "    # X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "Y_train=convert_one_hot(category)\n",
    "def train_nn(model,Y_train):\n",
    "    maxlen=100\n",
    "    tokenizer = Tokenizer(num_words=15000)\n",
    "\n",
    "    tokenizer.fit_on_texts(contents)\n",
    "\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(contents)\n",
    "    # X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "    model.fit(X_train,Y_train,batch_size=128,epochs=10)\n",
    "    return model\n",
    "def creat_model_cnn(vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, 50, input_length=100))\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(13,activation=\"softmax\"))\n",
    "    model.compile(optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "    return model\n",
    "def creat_model_lstm(vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, 50, input_length=100))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(256)))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(13,activation=\"softmax\"))\n",
    "    model.compile(optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "    return model\n",
    "model_category_cnn=creat_model_cnn(vocab_size)\n",
    "model_category_cnn=train_nn(model_category_cnn,Y_train)\n",
    "model_category_cnn.save(\"../models/CNN_category.h5\")\n",
    "model_category_lstm=creat_model_lstm(vocab_size)\n",
    "model_category_lstm=train_nn(model_category_cnn,Y_train)\n",
    "model_category_lstm.save(\"../models/LSTM_category.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FED1D3FB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001FED1D3FB88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "160/160 [==============================] - 20s 119ms/step - loss: 0.0916 - accuracy: 0.5659\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 20s 125ms/step - loss: 0.0741 - accuracy: 0.5831\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 19s 117ms/step - loss: 0.0610 - accuracy: 0.6598\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 19s 116ms/step - loss: 0.0482 - accuracy: 0.7380\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 18s 115ms/step - loss: 0.0381 - accuracy: 0.7936\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 19s 118ms/step - loss: 0.0335 - accuracy: 0.8141\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 18s 115ms/step - loss: 0.0300 - accuracy: 0.8252\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 19s 121ms/step - loss: 0.0288 - accuracy: 0.8277\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 19s 120ms/step - loss: 0.0260 - accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 19s 117ms/step - loss: 0.0258 - accuracy: 0.8420\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 18s 115ms/step - loss: 0.0263 - accuracy: 0.8346\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 19s 118ms/step - loss: 0.0258 - accuracy: 0.8401\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 19s 116ms/step - loss: 0.0254 - accuracy: 0.8365\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 19s 120ms/step - loss: 0.0248 - accuracy: 0.8420\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 21s 131ms/step - loss: 0.0241 - accuracy: 0.8429\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 22s 135ms/step - loss: 0.0236 - accuracy: 0.8444\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 20s 125ms/step - loss: 0.0235 - accuracy: 0.8441\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 21s 130ms/step - loss: 0.0232 - accuracy: 0.8460s\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 20s 124ms/step - loss: 0.0228 - accuracy: 0.8452\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 18s 115ms/step - loss: 0.0227 - accuracy: 0.8490\n"
     ]
    }
   ],
   "source": [
    "def creat_model_cnn_E(vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, 50, input_length=100))\n",
    "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(len(reactions)))\n",
    "    model.compile(optimizer='adam',\n",
    "                        loss='mse',\n",
    "                         metrics=['accuracy'])\n",
    "    return model\n",
    "def creat_model_lstm_E(vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, 50, input_length=100))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(256)))\n",
    "#     model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(len(reactions)))\n",
    "    model.compile(optimizer='adam',\n",
    "                        loss='mse',\n",
    "                         metrics=['accuracy'])\n",
    "    return model\n",
    "model_category_cnn=creat_model_cnn_E(vocab_size)\n",
    "model_category_cnn=train_nn(model_category_cnn,Emotions)\n",
    "model_category_cnn.save(\"../models/CNN_Emotion.h5\")\n",
    "model_category_lstm=creat_model_lstm_E(vocab_size)\n",
    "model_category_lstm=train_nn(model_category_cnn,Emotions)\n",
    "model_category_lstm.save(\"../models/LSTM_Emotion.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/tokenizer_NN.pickle\",\"wb\") as file:\n",
    "    pickle.dump(tokenizer,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def convert_one_hot(ytrain):\n",
    "#     encoder = LabelEncoder()\n",
    "    y_train_nn = encoder.transform(ytrain)\n",
    "    y_train_nn\n",
    "    encoders = OneHotEncoder(sparse=False)\n",
    "    y_train_nn = y_train_nn.reshape(len(y_train_nn), 1)\n",
    "    y_train_nn=encoders.fit_transform(y_train_nn)\n",
    "    return y_train_nn\n",
    "Y_train=convert_one_hot(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen=100\n",
    "tokenizer = Tokenizer(num_words=15000)\n",
    "\n",
    "tokenizer.fit_on_texts(contents)\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(contents)\n",
    "# X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "    # Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Pad sequences with zeros\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, 50, input_length=100))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(13,activation=\"softmax\"))\n",
    "model.compile(optimizer='adam',\n",
    "                loss='mse',\n",
    "                 metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,Y_train,batch_size=128,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN_category.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenizer_NN.pickle\",\"wb\") as file:\n",
    "    pickle.dump(tokenizer,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3491851e-03, 8.1063056e-04, 1.1725579e-03, 3.2122436e-04,\n",
       "        6.5769744e-04, 2.0318530e-03, 3.0358115e-04, 4.8772068e-05,\n",
       "        1.6519595e-03, 5.3208960e-06, 9.9078685e-01, 8.4135431e-04,\n",
       "        1.8980825e-05]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20057, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
